{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import process, utils, fuzz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exported_contacts_df = pd.read_csv('/Users/michaelbarski/Desktop/personal_projects/contact-company-matching-poc/making_new_data/new_exported_contacts.csv')\n",
    "company_data_df = pd.read_csv('/Users/michaelbarski/Desktop/personal_projects/contact-company-matching-poc/making_new_data/company_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note -- the company_data_df has added subdomains and typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts_urls = new_exported_contacts_df.company_url\n",
    "company_urls = company_data_df.company_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below I'll do the following:\n",
    "- write the regex pattern\n",
    "- select the pattern in the company_urls that contains only the base burl -- in this case match.group(2) is the url without a subdomain\n",
    "- replace the original value for company_url with match.group(2) to get rid of all of the subdomains\n",
    "- last, use our len(matches) to double check to make sure that we matched all of the records in the data frame (500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'(\\w+\\.)?(\\w+\\.com)')\n",
    "\n",
    "matches = []\n",
    "for val in company_urls:\n",
    "    match = pattern.fullmatch(val)\n",
    "    no_subdomain = match.group(2)\n",
    "    matches.append(no_subdomain)\n",
    "    company_data_df.company_url.replace({val:no_subdomain}, inplace=True)\n",
    "\n",
    "print(len(matches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'll merge the two dataframes using an inner join, matching on the company_url column that we just reformatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_matched_df = pd.merge(new_exported_contacts_df, company_data_df, how='inner', on='company_url')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was able to match 470/500 contacts to companies with the first merge by using regex (94%).\n",
    "We don't have a 100% match because we intentionally created typos for a number of urls (in an effort to simulate a messier dataset).\n",
    "\n",
    "There are many ways to match and merge the rest of the data, but here I'll showcase some approximate (or 'fuzzy') matching on company_name. This can be useful if there are no company_urls to match on--or, it can be useful if the urls on each seperate dataset are completely different.\n",
    "- (e.g. holidayinncommunityvillage.com vs hicv.com would be very challenging to match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I'll create a seperate list for any unmatched records. In the end I'll join these two seperate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_urls = url_matched_df.company_url\n",
    "unmatched_contacts_df = new_exported_contacts_df[~new_exported_contacts_df['company_url'].isin(matched_urls)].copy()\n",
    "unmatched_names_contacts = unmatched_contacts_df.company_name.unique()\n",
    "\n",
    "cd_company_names = list(company_data_df.company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanay, jeffrey a = chanay, jeffrey a esq\n",
      "morlong associates = morlong associates\n",
      "fbs business finance = fbs business finance\n",
      "vicon corporation = vicon corporation\n",
      "alpenlite = alpenlite inc\n",
      "beutelschies & company = beutelschies & company\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extracted_matches = []\n",
    "for name in unmatched_names_contacts:\n",
    "    fuzzy_match = process.extractOne(query=name, choices=cd_company_names, scorer=fuzz.partial_ratio, score_cutoff=100)\n",
    "    extracted_matches.append(fuzzy_match)\n",
    "    print(f'{name} = {fuzzy_match[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the output, we can see that everything worked In this scenario, only 2 of the names were different and all of the matches were correct ones. I've essentailly collected all of the approximate and exact matches in one go. With larger data unclean data sets this situation becomes more involved, but with a simple situation like this, fuzzy matching provides an elegant solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I could replace each name of the two names without a loop, but using the loop allows this process to be more automated when the list content changes, saving a little time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanay, jeffrey a chanay, jeffrey a esq\n",
      "morlong associates morlong associates\n",
      "fbs business finance fbs business finance\n",
      "vicon corporation vicon corporation\n",
      "alpenlite alpenlite inc\n",
      "beutelschies & company beutelschies & company\n"
     ]
    }
   ],
   "source": [
    "for i in (range(len(unmatched_names_contacts))):\n",
    "        to_replace = unmatched_names_contacts[i]\n",
    "        replace_with = extracted_matches[i][0]\n",
    "        print(to_replace,replace_with)\n",
    "        unmatched_contacts_df.company_name.replace(to_replace, replace_with, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create another df for the names we matched and reformat the columns so we can combine the two merged dfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_matched_df = pd.merge(unmatched_contacts_df,company_data_df, 'inner', on='company_name').drop('company_url_y', axis=1)\n",
    "name_matched_df.rename(columns={'company_url_x':'company_url'}, inplace=True)\n",
    "url_matched_df.drop(['company_name_y'], axis = 1, inplace=True)\n",
    "url_matched_df.rename(columns={'company_name_x':'company_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last we stack both of these dataframes on top of each other to create a complete fully_matched list of company/contact records. Then I rearrange the columns to make it look more uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_matched_df = pd.concat([url_matched_df, name_matched_df], axis=0)\n",
    "\n",
    "column_to_move = fully_matched_df.pop('company_name')\n",
    "fully_matched_df.insert(8, 'company_name', column_to_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_matched_df.to_csv('finished_contact_records.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We matched all of the records and merged them successfully! We've associated company data with every contact record. Now we can upload these new records to salesforce--or whichever crm we'd like--and our salespeople can see the company information (like revenue and what tech the company uses) for every contact; now it's all available in one place!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
